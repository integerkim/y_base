
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>2021 Y-BASE AI Symposium - Everything You Need to Know to Reproduce Upcoming Immersive METAVERSE Media: Quality of Experience (QoE) </title>

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
  <link href="css/style.css" rel="stylesheet" type="text/css" />
</head>

<body>

<div class="container">
  <table border="0" align="center">
    <tr>
      <td width="700" align="center" valign="middle"><h3>2021 Y-Base AI Symposium</h3>
      <span class="title">Everything You Need to Know to Reproduce Unpcoming Immersive METAVERSE Media: Quality of Experience (QoE)</span></td>
    </tr>
    <tr>
        <h2><td colspan="3" align="center"><br>
          Slides and recorded videos will be provided on this webpage.
          <br>
          Location: <a href="https://zoom.us/">https://zoom.us/</a><br>
          YouTube: <a href="https://www.youtube.com">https://www.youtube.com</a><br>
          Time: <b>0800-1100 - Half day, October 26 (Tuesday), 2021</b>
        </td>
        </h2>
    </tr>
  </table>
<!--   <p><img src="figures/teaser.jpg" width="1000" align="middle" /></p> -->
</div>

</br>


<div class="container">
  <h2>Overview</h2>
    <div class="schedule">
    <p>Today, the metaverse is a shared virtual space where people are represented by digital avatars (think Ready Player One). The virtual world constantly grows and evolves based on the decisions and actions of the society within it. Eventually, people will be able to enter the metaverse, completely virtual (i.e. with virtual reality) or interact with parts of it in their physical space with the help of augmented and mixed reality. However, making fully interactive 3D metaverses with high-quality video has never been easier. </p>
    <p>Therby, it is necessary to assess the quality of experience (QoE) perceived by users adaptive to those future multimedia systems. In this context, this tutorial aims at bridging together the current research advances with QoE assessment. Moreover, this tutorial will discuss how to evaluate the QoE for next generation multimedia from the human cognitive perspective, present the latest articles aligned with recent industrial trend, contents acquisition and quality assessment, and have technical exchanges.</p>
    </div>
</div>

</br>


<div class="container">
  <h2>Organizers</h2>
  <div>
    <div class="instructor">
          <a href="https://jw09191.github.io/about">
            <div class="instructorphoto"><img src="figures/test.jpeg"></div>
            <div>Jinwoo Kim<br>Ph.D. Candidate at Yonsei University<br><br></div>
          </a>
      </div>
      <div class="instructor"></div>
      <div class="instructor">
        <a href="https://jw09191.github.io/about">
            <div class="instructorphoto"><img src="figures/test.jpeg"></div>
            <div>Seonghwa Choi<br>Ph.D. Candidate at Yonsei University<br><br></div>
        </a>
      </div>
      <div class="instructor">
        <a href="https://jw09191.github.io/about">
            <div class="instructorphoto"><img src="figures/test.jpeg"></div>
            <div>Nguyen Anh Duc<br>Ph.D. Candidate at Yonsei Universitysd</div>
        </a>
      </div>
       <div class="instructor"></div>
      <div class="instructor">
        <a href="https://jw09191.github.io/about">
            <div class="instructorphoto"><img src="figures/test.jpeg"></div>
            <div>Nguyen Anh Duc<br>Ph.D. Candidate at Yonsei Universitysd</div>
        </a>
      </div>
  </div>
</div>

</br>



<div class="container">
  <h2>Tentative Schedule</h2>
    <div class="schedule">
      <p><span class="announce_date">08:00- am</span>. <b>Opening remarks</b> <em>Seoung Hwan Suh</em> </p>
      <p><span class="announce_date">08:10- am</span>. <b>Congratulatory Remarks remarks</b> <em>Hyesook Lim</em> </p>
      <p class="subtitle"><span class="announce_date">08:20- am</span>. <b>Building high performance chest x-ray classification models and understanding why they are all wrong.</b> <a href="https://jw09191.github.io/"><em>Presenter1</em></a>. <div style="margin-left:5em;">The high performance of modern computer vision methods has resulted in considerable interest in applications to radiology. To galvanize research in this area, a number of research groups have released large publicly available datasets, particularly for chest radiographs that benefit from large resources such as NIH ChestX-ray14, CheXpert, PadChest, and MIMIC-CXR. These images particularly benefit from a free-text interpretation provided by a practicing domain expert, which provides a human interpretable label of the image. However, caution must be taken when developing models using data acquired during routine clinical practice. A number of implicit biases exist: the acquisition of the image is based on clinical need, the interpretation of the image is a response to a specific clinical question, and the structuring of the data is not intended for retrospective research. In this tutorial, we will build high-performance computer vision models using large publicly available datasets. We will evaluate the performance of these classifiers on distinct institutions, and highlight generalization issues. We further use class-dependent model interpretation methods to inspect our classifier and highlight the source of its biases. We will end with suggestions for researchers who aim to build machine learning models on retrospectively collected clinical data. [<a href="">slides</a>] [<a href="https://youtube.com">recorded video</a>] </div></p>
      <p class="subtitle"><span class="announce_date">08:50- am</span>. <b>Deep learning for cardiovascular imaging applications.</b> <a href="https://jw09191.github.io/"> <em>Presenter2</em></a>. <div style="margin-left:5em;">Clinical practice has been revolutionized through advancements in non-invasive imaging modalities. In particular, the field of cardiovascular medicine has witnessed widespread adoption of computed tomography (CT), magnetic resonance imaging (MRI), echocardiography, and nuclear perfusion imaging for day-to-day clinical practice. The incorporation of an imaging approach in the diagnostic and prognostic process has allowed for the institution of precise, patient-oriented and disease-targeted cardiovascular therapies. Nevertheless, manual review and interpretation of increasing volumes of cardiovascular imaging studies have been associated with an inefficient workflow, significant time requirements as well as significant intra- and inter-reader variability. The aim of this session is to share the general methods by which deep learning models have been used for segmentation and classification tasks within the realm of cardiovascular imaging, with an emphasis on the advantages of incorporating artificial intelligence and deep learning within a clinical workflow. [<a href="">slides</a>] [<a href="https://youtu.be/b_Z8B9tSyJg">recorded video</a>] </div></p>
      <p class="subtitle"><span class="announce_date">09:20- am</span>. <b>Clinical context for medical imaging deep learning models: model fusion techniques to combine medical imaging with structured clinical data.</b> <a href="https://jw09191.github.io/"> <em>Presenter3</em></a>. <div style="margin-left:5em;">Advancements in computer vision and deep learning techniques carry the potential to make significant contributions to healthcare, particularly in fields that utilize medical imaging for diagnosis, prognosis, and treatment decisions. The current state of the art deep learning models for automated diagnosis and outcome prediction using medical imaging tend not to consider patient electronic medical records and clinical variables. Pertinent and accurate information regarding the current symptoms and past medical history enables physicians to interpret imaging findings in the appropriate clinical context, leading to higher diagnostic accuracy, better clinical predictions, and ideally a better outcome for the patient. Deep learning models that use images without clinical context will ultimately be limited, just as physicians cannot be effective without the knowledge of the patient's medical records. In this session, we describe different fusion techniques applied to combine medical imaging with other clinical data, and systematically review literature in this field. We will also present current knowledge and summarize the key insights that researchers can use to make advancements in deep learning for medicine. [<a href="">slides</a>] [<a href="https://youtube.com">recorded video</a>] </div></p>
      <p class="subtitle"><span class="announce_date">09:50- am</span>. <b>Natural language processing on radiology reports to generate large labeled dataset.</b> <a href="https://jw09191.github.io/"> <em>Presenter4</em></a>. <div style="margin-left:5em;">Healthcare institutions have millions of imaging studies associated with unstructured free text radiology reports that describe imaging findings in the radiologist's language who read the study. But, there are no reliable methods for leveraging these reports to create structured labels for training deep learning models. We will present semantic word embedding methods trained on a large cohort of narrative reports and can mine information from free-text radiology reports. It has been successfully applied to hemorrhage and pulmonary embolism risk assessment, liver, breast, and brain tumor categorization with minimal task-specific tuning. The method also presents robustness towards new domain adaptation. Besides radiology reports, the method has also been applied to other types of clinical notes (e.g., progress reports, discharge summary) for extracting information about patient-centered outcomes and distant cancer recurrence status. Our proposed methods outperform domain-specific rule-based systems that need a tremendous amount of hand-engineering in every domain that we tested. [<a href="">slides</a>] [<a href="https://youtube.com">recorded video</a>] </div></p>
      <p class="subtitle"><span class="announce_date">10:00-11:00 am</span>. <b>Interpretable deep learning model for multiple modal and cross-domain medical images.</b> <a href="https://jw09191.github.io/"> <em>Presenter5</em></a>. <div style="margin-left:5em;">With the rapid growth in the number of imaging procedures, digitization of images, internet explosion, and healthcare's inexorable digital migration, we face the challenges of developing machine learning algorithms to analyze these multiple modal and different domain medical images efficiently and improve the current healthcare system workflow. In this session, we will present our current work on improving the generalization ability of deep learning models on medical images from different domains: Non-contrast CT, contrast-enhanced CT, MRI, chest X-ray using unsupervised image translation model for domain adaptation and data augmentation purpose. Secondly, we will present the recent work on combining multiple modal data:  the labeled and unlabeled imaging data, clinical records, and clinical notes to improve current computer-aided diagnosis systems using semi-supervised graph models to reduce the imaging labeling expense and improve diagnosis performance. Since the interpretation of deep learning models on medical imaging problems is very important, we will present our work on developing interpretable chest x-ray diagnosis systems with adversary imaging decomposition and synthesis framework. [<a href="">slides</a>] [<a href="https://www.youtube.com">recorded video</a>] </div></p>
    </div>
</div>

<div class="container">
  <h2>About the speakers</h2>
    <div class="schedule">
        <p><b>Jinwoo Kim</b>, received the B.S degree in electrical and electronic from Hongik University, Seoul, South Korea, in 2016. He is currently pursuing the M.S and Ph.D. degrees with Multi-Dimensional Insight Laboratory, Yonsei University. His current research interests include 2D/3D image and video processing based on human visual system, quality assessment of 2D/3D image and video, 3D computer vision and deep learning.</p>
        <p><b>Woojae Kim</b>, received the B.S. degree in electronic engineering from Soongsil University, Seoul, South Korea, in 2015. He is currently pursuing the M.S. and Ph.D. degrees with the Multi-Dimensional Insight Laboratory, Yonsei University. He was a Research Assistant under the guidance of Prof. Weisi Lin with the Laboratory for School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore in 2018. His research interests include image and video processing based on the human visual system, image/video quality assessment, computer vision, and machine learning.</p>
        <p><b>Seonghwa Choi</b>, received the B.S. degree in electronic engineering from Soongsil University, Seoul, South Korea, in 2018. He is currently pursuing the M.S. and Ph.D. degrees with the Multi-Dimensional Insight Laboratory, Yonsei University. His research interests include image and video processing based on the human visual system, computer vision, and machine learning.</p>
    </div>
</div>

</br>

<div class="containersmall">
    <p>Please contact <a href="jw09191@yonsei.ac.kr">Jinwoo Kim</a> if you have question.</p>
</div>

<!--<p align="center" class="acknowledgement">Last updated: Jan. 6, 2017</p>-->
</body>
</html>

